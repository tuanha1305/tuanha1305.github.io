---
title: D·ª± √°n c√° nh√¢n - Nghi√™n c·ª©u v√† t√¨m hi·ªÉu train/finetune ·ª©ng d·ª•ng LLMs
categories: [project]
---

## **T·ªïng quan**  
D·ª± √°n t·∫≠p trung v√†o vi·ªác kh√°m ph√° quy tr√¨nh **hu·∫•n luy·ªán (training)** v√† **tinh ch·ªânh (finetuning)** c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) nh∆∞ GPT, Llama 2, BERT... cho c√°c b√†i to√°n c·ª• th·ªÉ. M·ª•c ti√™u bao g·ªìm:  
- Hi·ªÉu r√µ ki·∫øn tr√∫c v√† c∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa LLMs  
- Th·ª±c h√†nh train/finetune tr√™n nhi·ªÅu lo·∫°i d·ªØ li·ªáu kh√°c nhau  
- ·ª®ng d·ª•ng v√†o c√°c t√°c v·ª• th·ª±c t·∫ø: **Chatbot, Text Summarization, Question Answering**  

## **C√¥ng ngh·ªá s·ª≠ d·ª•ng**  
- **Ng√¥n ng·ªØ**: Python  
- **Framework**: PyTorch, HuggingFace Transformers, TensorFlow  
- **M√¥ h√¨nh**: GPT-2, Llama 2, BERT, T5  
- **Ph·∫ßn c·ª©ng**: Google Colab (GPU T4) 

## **Quy tr√¨nh th·ª±c hi·ªán**  

### **1. Chu·∫©n b·ªã d·ªØ li·ªáu**  
- **Thu th·∫≠p d·ªØ li·ªáu**: Crawl t·ª´ Wikipedia, OpenWebText, ho·∫∑c s·ª≠ d·ª•ng dataset c√≥ s·∫µn (CNN/DailyMail cho summarization, SQuAD cho QA).  
- **Ti·ªÅn x·ª≠ l√Ω**:  
  - L√†m s·∫°ch (remove special chars, normalize text)  
  - Tokenization (s·ª≠ d·ª•ng tokenizer c·ªßa t·ª´ng model)  
  - Chia train/val/test (t·ªâ l·ªá 80/10/10)  

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
```

### **2. Hu·∫•n luy·ªán t·ª´ ƒë·∫ßu (Pretraining)**  
- **Ki·∫øn tr√∫c model**: L·ª±a ch·ªçn gi·ªØa **GPT (Decoder-only)** ho·∫∑c **BERT (Encoder-only)** t√πy b√†i to√°n.  
- **Fine-tuning Task**:  
  - **Causal LM** (GPT): D·ª± ƒëo√°n t·ª´ ti·∫øp theo ‚Üí ph√π h·ª£p **text generation**.  
  - **Masked LM** (BERT): D·ª± ƒëo√°n t·ª´ b·ªã che ‚Üí ph√π h·ª£p **classification, QA**.  

```python
from transformers import GPT2LMHeadModel
model = GPT2LMHeadModel.from_pretrained("gpt2")
optimizer = AdamW(model.parameters(), lr=5e-5)
```

### **3. Tinh ch·ªânh (Finetuning)**  
- **Ph∆∞∆°ng ph√°p**:  
  - **Full Finetuning**: C·∫≠p nh·∫≠t to√†n b·ªô weights c·ªßa model.  
  - **LoRA (Low-Rank Adaptation)**: Ch·ªâ train th√™m m·ªôt s·ªë layer ‚Üí ti·∫øt ki·ªám b·ªô nh·ªõ.  
- **·ª®ng d·ª•ng**:  
  - **Chatbot**: Finetune GPT tr√™n dataset ch·ª©a h·ªôi tho·∫°i.  
  - **Text Summarization**: Finetune T5/BART tr√™n dataset CNN/DailyMail.  

```python
# V√≠ d·ª• finetune T5 ƒë·ªÉ summarization
from transformers import T5ForConditionalGeneration
model = T5ForConditionalGeneration.from_pretrained("t5-small")
inputs = tokenizer("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
```

### **4. ƒê√°nh gi√° hi·ªáu su·∫•t**

- **Metric**:  
  - **Perplexity** (cho language model)  
  - **BLEU, ROUGE** (cho summarization)  
  - **F1, Accuracy** (cho classification)  
- **So s√°nh model**:

  | Model       | Dataset       | Perplexity ‚Üì | BLEU-4 ‚Üë |  
  |-------------|---------------|--------------|----------|  
  | GPT-2 (base)| OpenWebText   | 25.3         | -        |  
  | GPT-2 (ft)  | Custom Chat   | **18.7**     | -        |  
  | T5 (ft)     | CNN/DailyMail | -            | **22.1** |  

### **5. T·ªëi ∆∞u h√≥a & Tri·ªÉn khai**  
- **T·ªëi ∆∞u**:  
  - **Quantization**: Gi·∫£m model size b·∫±ng FP16/INT8.  
  - **ONNX Runtime**: TƒÉng t·ªëc inference.  
- **Tri·ªÉn khai**:  
  - **Streamlit** cho demo chatbot.  
  - **FastAPI** ƒë·ªÉ t·∫°o API service.  

```python
# Export sang ONNX
torch.onnx.export(model, inputs, "model.onnx", opset_version=11)
```

## **K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c**  
‚úÖ Train th√†nh c√¥ng **GPT-2** t·ª´ ƒë·∫ßu tr√™n dataset 10GB text.  
‚úÖ Finetune **T5** cho b√†i to√°n summarization ƒë·∫°t **BLEU-4 = 22.1**.  
‚úÖ X√¢y d·ª±ng **Chatbot** c√≥ kh·∫£ nƒÉng h·ªôi tho·∫°i t·ª± nhi√™n.  

## **B√†i h·ªçc kinh nghi·ªám**  
üîπ **D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng** quan tr·ªçng h∆°n k√≠ch th∆∞·ªõc model.  
üîπ **LoRA/P-Tuning** gi√∫p finetune hi·ªáu qu·∫£ v·ªõi √≠t t√†i nguy√™n.  
üîπ C·∫ßn **ƒë√°nh gi√° nhi·ªÅu metric** ƒë·ªÉ hi·ªÉu r√µ ∆∞u/nh∆∞·ª£c ƒëi·ªÉm t·ª´ng model.  

## **H∆∞·ªõng ph√°t tri·ªÉn**  
‚û°Ô∏è Th·ª≠ nghi·ªám **Llama 2, Falcon** tr√™n multi-task learning.  
‚û°Ô∏è ·ª®ng d·ª•ng **RAG (Retrieval-Augmented Generation)** ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c.  

**GitHub**: [Link repo d·ª± √°n]()